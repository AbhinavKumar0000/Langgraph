{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cffd606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Codes\\Learning\\Lang_graph\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20e942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c067ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state\n",
    "\n",
    "class LLMState(TypedDict):\n",
    "    ques: str\n",
    "    ans: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4271af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_question(state: LLMState) -> LLMState:\n",
    "    #extracting the ques from state\n",
    "    question = state[\"ques\"]\n",
    "\n",
    "    #prompt\n",
    "    prompt = f\"Answer the following question: {question}\"\n",
    "\n",
    "    #llm feeding\n",
    "    response = model.invoke([prompt])\n",
    "\n",
    "    #updating hte state\n",
    "    state[\"ans\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0b4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(LLMState)\n",
    "\n",
    "graph.add_node(\"llm_qa\", llm_question)\n",
    "\n",
    "graph.add_edge(START, \"llm_qa\")\n",
    "graph.add_edge(\"llm_qa\", END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70421804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ques': 'What is LangGraph?', 'ans': 'LangGraph is a Python library designed to make it easier to build **stateful, multi-actor conversational applications** using Large Language Models (LLMs). Think of it as a framework for creating complex, interactive AI agents that can reason, plan, and act over time.\\n\\nHere\\'s a breakdown of its key aspects:\\n\\n*   **Stateful Conversations:** Unlike simple prompt-response chains, LangGraph allows you to maintain and update the \"state\" of a conversation over multiple turns. This state can include things like user preferences, intermediate results, or goals the agent is trying to achieve.\\n\\n*   **Nodes and Edges:** LangGraph represents your application as a graph of interconnected nodes. Each node represents a step in the conversation flow, such as:\\n    *   **LLM Calls:** Querying a language model for information or to generate text.\\n    *   **Tools:** Using external tools like search engines, calculators, or databases.\\n    *   **Conditional Logic:** Making decisions based on the current state or user input.\\n    *   **Human Input:** Requesting information or feedback from a human.\\n\\n    The edges define how the conversation flows from one node to another.\\n\\n*   **Actors:** The nodes can be thought of as \"actors\" in the conversation. Each actor has a specific role and responsibility.\\n\\n*   **Loops and Control Flow:** LangGraph supports creating loops and complex control flow within your conversational application. This allows you to build agents that can iterate, retry actions, or explore different paths based on the current state.\\n\\n*   **Integration with LangChain:** LangGraph is tightly integrated with LangChain, a popular library for building LLM-powered applications. You can easily use LangChain components like prompts, models, and chains within your LangGraph nodes.\\n\\n**In simpler terms:**\\n\\nImagine you\\'re building a customer service chatbot. With LangGraph, you can:\\n\\n1.  Start with an initial state (e.g., \"customer is asking for help\").\\n2.  Create a node that asks the user what they need help with.\\n3.  Based on their response, route the conversation to different nodes (e.g., \"troubleshooting technical issues\" or \"processing a return\").\\n4.  Update the state as the conversation progresses (e.g., \"customer\\'s order number is known,\" \"issue has been identified\").\\n5.  Loop back to previous nodes if more information is needed.\\n\\n**Key Benefits of using LangGraph:**\\n\\n*   **Improved Conversational Flow:**  More natural and engaging conversations compared to simple prompt-response systems.\\n*   **Complex Logic:** Ability to implement sophisticated reasoning and decision-making processes.\\n*   **Reusability:**  Graph-based structure makes it easier to reuse and modify components.\\n*   **Maintainability:**  Clear separation of concerns and well-defined control flow.\\n\\n**In summary, LangGraph provides a powerful and flexible framework for building sophisticated, stateful conversational applications with LLMs, allowing you to create more intelligent and interactive AI agents.**'}\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "initial_state = {\"ques\": \"What is LangGraph?\"}\n",
    "\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbae948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
